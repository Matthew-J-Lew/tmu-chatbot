# Docker version
# version: "3.9"

# Each service is a container running a piece of software
services:

  # pg container, posgres with pgvector extension for the rag database
  pg:
    image: pgvector/pgvector:pg16
    # basic db stuff for now, maybe change later
    environment:
      POSTGRES_DB: ragdb
      POSTGRES_USER: rag
      POSTGRES_PASSWORD: rag
    ports: ["5432:5432"]
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./app/ingestion/schema.sql:/docker-entrypoint-initdb.d/01_schema.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag -d ragdb"]
      interval: 5s
      timeout: 3s
      retries: 20

  # Ollama container, runs the LLM server (maybe switch to openai later)
  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]   # <- use the built-in CLI instead of curl
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s

  # API container, will connect to the db and LLM later on 
  api:
    build: ./app/api
    environment:
      PGHOST: pg
      PGUSER: rag
      PGPASSWORD: rag
      PGDATABASE: ragdb
      OLLAMA_HOST: http://ollama:11434
    depends_on:
      pg:
        condition: service_healthy
      ollama:
        condition: service_healthy
    ports: ["8000:8000"]

  # Ingestion container for our data pipeline
  ingestion:
    build: ./app/ingestion
    profiles: ["ingest"]
    depends_on:
      pg:
        condition: service_healthy

volumes:
  pgdata:
  ollama:
