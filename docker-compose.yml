services:
  pg:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: ragdb
      POSTGRES_USER: rag
      POSTGRES_PASSWORD: rag
    ports: ["5432:5432"]
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./app/ingestion/schema.sql:/docker-entrypoint-initdb.d/01_schema.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rag -d ragdb"]
      interval: 5s
      timeout: 3s
      retries: 20

  ollama:
    image: ollama/ollama:latest
    ports: ["11434:11434"]
    volumes:
      - ollama:/root/.ollama
    environment:
      # Keep the model loaded longer to reduce cold-start latency
      OLLAMA_KEEP_ALIVE: 30m
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s

  # Redis for production caching
  redis:
    image: redis:7-alpine
    ports: ["6379:6379"]
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 30

  api:
    build:
      context: .
      dockerfile: app/api/Dockerfile
    environment:
      # DB
      PGHOST: pg
      PGUSER: rag
      PGPASSWORD: rag
      PGDATABASE: ragdb
      PGPORT: 5432

      # Ollama
      OLLAMA_HOST: http://ollama:11434
      OLLAMA_MODEL: qwen2.5:1.5b

      # LLM provider switch
      # - azure  => Azure OpenAI (recommended)
      # - ollama => local Ollama container (fallback)
      LLM_PROVIDER: ${LLM_PROVIDER:-ollama}
      # Optional: if set, and primary provider fails, attempt fallback once.
      LLM_FALLBACK_PROVIDER: ${LLM_FALLBACK_PROVIDER:-}

      # Azure OpenAI (values can live in .env)
      AZURE_OPENAI_ENDPOINT: ${AZURE_OPENAI_ENDPOINT:-https://arts-chatbot-openai.openai.azure.com/}
      AZURE_OPENAI_API_KEY: ${AZURE_OPENAI_API_KEY:-}
      AZURE_OPENAI_DEPLOYMENT: ${AZURE_OPENAI_DEPLOYMENT:-gpt-4o-mini}
      AZURE_OPENAI_API_VERSION: ${AZURE_OPENAI_API_VERSION:-2024-10-21}
      # Cost controls
      AZURE_OPENAI_MAX_TOKENS: ${AZURE_OPENAI_MAX_TOKENS:-512}
      AZURE_OPENAI_TEMPERATURE: ${AZURE_OPENAI_TEMPERATURE:-0.1}

      # Redis
      REDIS_URL: redis://redis:6379/0

      # Production tuning knobs
      RAG_TOP_K: 4
      RAG_NUM_CANDIDATES: 12
      RERANK_ENABLED: "true"

      # Prompt limits
      MAX_CHUNK_CHARS: 1600
      MAX_CONTEXT_CHARS: 8000

      # Generation limits
      OLLAMA_NUM_PREDICT: 512
      OLLAMA_TEMPERATURE: 0.1
      OLLAMA_TOP_P: 0.9

      # Reliability
      OLLAMA_TIMEOUT_SECONDS: 90
      OLLAMA_MAX_RETRIES: 2

      # Concurrency
      MAX_CONCURRENT_LLM: 2

      # Cache TTLs (seconds)
      CACHE_TTL_RESPONSE: 3600
      CACHE_TTL_RETRIEVAL: 3600
    depends_on:
      pg:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports: ["8000:8000"]

  # One-shot crawler that discovers URLs and writes approved targets to Postgres
  crawler:
    build:
      context: .
      dockerfile: app/crawler/Dockerfile
    profiles: ["crawl"]
    environment:
      PGHOST: pg
      PGUSER: rag
      PGPASSWORD: rag
      PGDATABASE: ragdb
      CRAWL_PROFILE: arts
      # Politeness: requests per second per host
      CRAWL_RPS: 1.0
      CRAWL_ENABLE_SITEMAPS: "true"
    depends_on:
      pg:
        condition: service_healthy

  # One-shot ingestion runner; set INGEST_MODE=db to ingest from crawl_targets
  ingestion:
    build:
      context: .
      dockerfile: app/ingestion/Dockerfile
    profiles: ["ingest"]
    environment:
      PGHOST: pg
      PGUSER: rag
      PGPASSWORD: rag
      PGDATABASE: ragdb
      INGEST_MODE: db
      CRAWL_PROFILE: arts
      INGEST_LIMIT: 200
      # Playwright (JS-rendering) ingestion options
      INGEST_USE_PLAYWRIGHT: ${INGEST_USE_PLAYWRIGHT:-true}
      INGEST_PLAYWRIGHT_ALWAYS: ${INGEST_PLAYWRIGHT_ALWAYS:-false}
      INGEST_PLAYWRIGHT_FALLBACK: ${INGEST_PLAYWRIGHT_FALLBACK:-true}
      INGEST_MIN_EXTRACTED_CHARS: ${INGEST_MIN_EXTRACTED_CHARS:-400}
      PLAYWRIGHT_EXPAND_ACCORDIONS: ${PLAYWRIGHT_EXPAND_ACCORDIONS:-true}
    depends_on:
      pg:
        condition: service_healthy

  # Optional: a simple "scheduler" container that runs crawl+ingest on an interval
  pipeline:
    build:
      context: .
      dockerfile: app/pipeline/Dockerfile
    profiles: ["pipeline"]
    environment:
      PGHOST: pg
      PGUSER: rag
      PGPASSWORD: rag
      PGDATABASE: ragdb
      PIPELINE_PROFILES: arts
      PIPELINE_INTERVAL_SECONDS: 21600 # 6h
      CRAWL_RPS: 1.0
      CRAWL_ENABLE_SITEMAPS: "true"
      INGEST_LIMIT: 200
      # Playwright (JS-rendering) ingestion options
      INGEST_USE_PLAYWRIGHT: ${INGEST_USE_PLAYWRIGHT:-true}
      INGEST_PLAYWRIGHT_ALWAYS: ${INGEST_PLAYWRIGHT_ALWAYS:-false}
      INGEST_PLAYWRIGHT_FALLBACK: ${INGEST_PLAYWRIGHT_FALLBACK:-true}
      INGEST_MIN_EXTRACTED_CHARS: ${INGEST_MIN_EXTRACTED_CHARS:-400}
      PLAYWRIGHT_EXPAND_ACCORDIONS: ${PLAYWRIGHT_EXPAND_ACCORDIONS:-true}
    depends_on:
      pg:
        condition: service_healthy

  adminer:
    image: adminer:latest
    restart: always
    ports:
      - 8080:8080
    depends_on:
      - pg

volumes:
  pgdata:
  ollama:
